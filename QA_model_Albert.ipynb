{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QA model_Albert.ipynb",
      "private_outputs": true,
      "provenance": [],
      "background_execution": "on",
      "authorship_tag": "ABX9TyPEi/55pfmBs34BTVxb/+ba",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YoonJiHwan98/Goorm_NLP/blob/main/QA_model_Albert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5wiDwOX2ClK"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install konlpy"
      ],
      "metadata": {
        "id": "ED09458QTAJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#!wandb login --relogin\n",
        "#% env WANDB_PROJECT=klue-mrc\n",
        "#% env WANDB_ENTITY=team5_groom\n",
        "\n",
        "#eaa023049260e35a4fe8a7ff980d18c15934e74a\n"
      ],
      "metadata": {
        "id": "Ia4r34iB_Yqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izTJhKcM1jsN"
      },
      "outputs": [],
      "source": [
        "# 라이브러리 임포트\n",
        "\n",
        "import pandas as pd\n",
        "import sys\n",
        "from tqdm import tqdm, trange\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import wandb\n",
        "from transformers import AdamW\n",
        "from statistics import mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zj8GBeHdAgIq"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6DjAJ_qvYmd"
      },
      "source": [
        "구글 드라이브 공유폴더 사용하기\n",
        "\n",
        "https://sundries-in-myidea.tistory.com/96"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vfUg0x4utAQ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpkOCwlNAPyU"
      },
      "source": [
        "# Loading Data Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WsaQex_uGEU"
      },
      "outputs": [],
      "source": [
        "#%%\n",
        "from typing import List, Tuple, Dict, Any\n",
        "import json\n",
        "import random\n",
        "import re\n",
        "\n",
        "\n",
        "class KoMRC:\n",
        "    def __init__(self, data, indices: List[Tuple[int, int, int]]):\n",
        "        self._data = data\n",
        "        self._indices = indices\n",
        "\n",
        "    # Json을 불러오는 메소드\n",
        "    @classmethod\n",
        "    def load(cls, file_path: str):\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as fd:\n",
        "            data = json.load(fd)\n",
        "\n",
        "        indices = []\n",
        "        for d_id, document in enumerate(data[\"data\"]):\n",
        "            for p_id, paragraph in enumerate(document[\"paragraphs\"]):\n",
        "                for q_id, _ in enumerate(paragraph[\"qas\"]):\n",
        "                    indices.append((d_id, p_id, q_id))\n",
        "\n",
        "        return cls(data, indices)\n",
        "\n",
        "    # 데이터 셋을 잘라내는 메소드\n",
        "    @classmethod\n",
        "    def split(cls, dataset, eval_ratio: float = 0.1, seed=42):\n",
        "        indices = list(dataset._indices)\n",
        "        random.seed(seed)\n",
        "        random.shuffle(indices)\n",
        "        train_indices = indices[int(len(indices) * eval_ratio) :]\n",
        "        eval_indices = indices[: int(len(indices) * eval_ratio)]\n",
        "\n",
        "        return cls(dataset._data, train_indices), cls(dataset._data, eval_indices)\n",
        "\n",
        "    def __getitem__(self, index: int) -> Dict[str, Any]:\n",
        "        d_id, p_id, q_id = self._indices[index]\n",
        "        paragraph = self._data[\"data\"][d_id][\"paragraphs\"][p_id]\n",
        "        d_id, p_id, q_id = self._indices[index]\n",
        "        \n",
        "        # \\n, \\n\\n 을 찾아서 공백으로 처리\n",
        "        # \\n을 찾기 때문에 \\n\\n인 경우 공백이 2개 생김.....\n",
        "        # tokenization했을 때 차이가 있는지 확인...\n",
        "        p = re.compile(\"[\\\\n]\")\n",
        "        # p = re.compile(\"[#:^$@*※~&ㆍ!』\\\"\\'\\\\n…\\○]\")\n",
        "        context = paragraph[\"context\"]\n",
        "        context = p.sub(\" \", context)\n",
        "        qa = paragraph[\"qas\"][q_id]\n",
        "\n",
        "        \"\"\"\n",
        "        guid 부분\n",
        "        train dataset -- guid과\n",
        "        ai-hub train dataset -- context_id 이 다름\n",
        "        \"\"\"\n",
        "        # context_id 가 qas 안에 key 형태로 있음\n",
        "        if paragraph.get(\"context_id\"):\n",
        "            guid = str(paragraph[\"context_id\"])\n",
        "        else:\n",
        "            guid = qa[\"guid\"]\n",
        "\n",
        "        question = qa[\"question\"]\n",
        "\n",
        "        # answers에 필요한 것만 추출\n",
        "        if isinstance(qa['answers'], dict):\n",
        "            answers = [{\n",
        "                \"text\": qa[\"answers\"][\"text\"],\n",
        "                \"answer_start\": qa[\"answers\"][\"answer_start\"],\n",
        "            }]\n",
        "        else:\n",
        "            answers = qa[\"answers\"]\n",
        "\n",
        "        return {\n",
        "            \"guid\": guid,\n",
        "            \"context\": context,\n",
        "            \"question\": question,\n",
        "            \"answers\": answers,\n",
        "        }\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self._indices)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fRS3Xgb91q_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzhaYhhNuGEX"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1h9I90lvo12"
      },
      "outputs": [],
      "source": [
        "# file_path = \"./drive/MyDrive/구름 자연어처리 프로젝트 공유 폴더/Reading Comprehension/data/TL_span_extraction_short.json\"\n",
        "file_path = \"./drive/MyDrive/구름 자연어처리 프로젝트 공유 폴더/Reading Comprehension/data/train.json\"\n",
        "\n",
        "dataset = KoMRC.load(file_path)\n",
        "# print(\"Number of Samples:\", len(dataset))\n",
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiB1nbOGByk2"
      },
      "outputs": [],
      "source": [
        "train_dataset, dev_dataset = KoMRC.split(dataset)\n",
        "print(\"Number of Train Samples:\", len(train_dataset))\n",
        "print(\"Number of Dev Samples:\", len(dev_dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0yHx6a8ecmR"
      },
      "outputs": [],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "W1itkgHAJZlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8GhmEJ-ANXH"
      },
      "source": [
        "# Preprocessing the Training Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "% pip install sentencepiece"
      ],
      "metadata": {
        "id": "brjWOj_drzta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWdhfjSWd2cw"
      },
      "outputs": [],
      "source": [
        "from transformers import AlbertTokenizer , AutoTokenizer\n",
        "\n",
        "model_name = \"albert-base-v2\" ## token 해보기 \n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n",
        "tokenizer.model_max_length = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoagNImmkJaE"
      },
      "outputs": [],
      "source": [
        "#data preferation \n",
        "train_context = [train_dataset[i]['context'] for i in range(len(train_dataset))]\n",
        "train_question = [train_dataset[i]['question'] for i in range(len(train_dataset))]\n",
        "train_answers = [train_dataset[i]['answers'] for i in range(len(train_dataset))]\n",
        "\n",
        "# train_encodings = tokenizer(train_question, train_context, max_length=1024, truncation=True, padding=\"max_length\", return_token_type_ids=True)\n",
        "train_encodings = tokenizer(train_question, train_context, truncation=\"only_second\", padding=\"max_length\", return_tensors='pt')\n",
        "\n",
        "\n",
        "dev_context = [dev_dataset[i]['context'] for i in range(len(dev_dataset))]\n",
        "dev_question = [dev_dataset[i]['question'] for i in range(len(dev_dataset))]\n",
        "dev_answers = [dev_dataset[i]['answers'] for i in range(len(dev_dataset))]\n",
        "\n",
        "dev_encodings = tokenizer(dev_question, dev_context, truncation=\"only_second\", padding=\"max_length\", return_tensors='pt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQNGMFh4okQx"
      },
      "outputs": [],
      "source": [
        "# 정답에 들어가는 데이터는 start, end position.\n",
        "\n",
        "def add_end_idx(answers, contexts):\n",
        "    for answer, context in zip(answers, contexts):\n",
        "        # 모델 학습을 위해 정답 데이터를 만들겠습니다.\n",
        "        # 정답 데이터는 start음절과 end 음절로 구성되어 있습니다.\n",
        "        # 모델은 전체 토큰 중에서 start token과 end token을 찾아내는 것을 목표로 학습하게 됩니다.\n",
        "        gold_text = answer[0]['text']\n",
        "        start_idx = answer[0]['answer_start']\n",
        "        end_idx = start_idx + len(gold_text)\n",
        "        \n",
        "\n",
        "        # sometimes squad answers are off by a character or two – fix this\n",
        "        # 실제 본문에서 해당 음절 번호로 잘라냈을 때, 정답과 같은지 검사해서 start, end를 보정합니다 :-)\n",
        "        # '이순신은 조선 중기의 무신이다' -> '이순신' -> start: 0, end: 4\n",
        "        if context[start_idx:end_idx] == gold_text:\n",
        "            answer[0]['answer_end'] = end_idx\n",
        "        elif context[start_idx-1:end_idx-1] == gold_text:\n",
        "            answer[0]['answer_start'] = start_idx - 1\n",
        "            answer[0]['answer_end'] = end_idx - 1     # When the gold label is off by one character\n",
        "        elif context[start_idx-2:end_idx-2] == gold_text:\n",
        "            answer[0]['answer_start'] = start_idx - 2\n",
        "            answer[0]['answer_end'] = end_idx - 2     # When the gold label is off by two characters\n",
        "    return answers\n",
        "\n",
        "train_answers = add_end_idx(train_answers, train_context)\n",
        "dev_answers = add_end_idx(dev_answers, dev_context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kkul0KS4Qi89"
      },
      "outputs": [],
      "source": [
        "train_answers[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3064Euwho8Ph"
      },
      "outputs": [],
      "source": [
        "# 기계 독해의 핵심부분.\n",
        "# 우리가 원하는건 음절단위로 읽는거지만 bert tokenizer는 wordpiece 단위.\n",
        "# 그래서 음절 단위에 있는 숫자를 tokenindex로 바꿔줘야 함.\n",
        "# 그래야지 해당 token index가 정답label의 시작이다. 라는 걸 알 수 있다.\n",
        "\n",
        "def add_token_positions(encodings, answers):\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "    # 이제 음절 index를 token index와 mapping하는 작업을 해보도록 하겠습니다 :-)\n",
        "    for i in range(len(answers)):\n",
        "        # tokenizer의 char_to_token 함수를 호출하면 음절 숫자를 token index로 바꿔줄 수 있습니다.\n",
        "        start_positions.append(encodings.char_to_token(i, answers[i][0]['answer_start'], len(train_encodings['input_ids'][0])))\n",
        "        end_positions.append(encodings.char_to_token(i, answers[i][0]['answer_end'], len(train_encodings['input_ids'][0])))\n",
        "        \n",
        "        # 아래 부분은 truncation을 위한 과정입니다.\n",
        "        # if start position is None, the answer passage has been truncated\n",
        "        if start_positions[-1] is None:\n",
        "            start_positions[-1] = tokenizer.model_max_length\n",
        "\n",
        "        # if end position is None, the 'char_to_token' function points to the space before the correct token - > add + 1\n",
        "        if end_positions[-1] is None:\n",
        "            end_positions[-1] = encodings.char_to_token(i, answers[i][0]['answer_end'] + 1)\n",
        "\n",
        "        # 추가된 예외 처리, 예를들어서 tokenizer와 model input의 max_length가 512인데, start와 end position이 600과 610 이면 둘다 max_length로 변경해야함.\n",
        "        # 어차피 max_length가 512인 모델은 정답을 볼 수 없음.  \n",
        "        ## 길이가 길 경우 대비해서 이를 나눈다. 그래서 token , strider 옵션 \n",
        "\n",
        "        if start_positions[-1] is None or start_positions[-1] > tokenizer.model_max_length: \n",
        "            start_positions[-1] = tokenizer.model_max_length\n",
        "        \n",
        "        if end_positions[-1] is None or end_positions[-1] > tokenizer.model_max_length:\n",
        "            end_positions[-1] = tokenizer.model_max_length\n",
        "\n",
        "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
        "    return encodings\n",
        "\n",
        "train_encodings = add_token_positions(train_encodings, train_answers)\n",
        "dev_encodings = add_token_positions(dev_encodings, dev_answers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbSL0WHkzFTp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "train_dataset_encoding = MyDataset(train_encodings)\n",
        "dev_dataset_encoding = MyDataset(dev_encodings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hUgJg864Ada"
      },
      "outputs": [],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0dOaxyvz-9z"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQ0N5H3yd9HZ"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "accumulation = 4\n",
        "\n",
        "train_loader = DataLoader(train_dataset_encoding, batch_size=batch_size//accumulation, shuffle=True, num_workers=0)\n",
        "dev_loader = DataLoader(dev_dataset_encoding, batch_size=batch_size//accumulation, shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenizedKoMRC(KoMRC):  \n",
        "    def __init__(self, data, indices: List[Tuple[int, int, int]]) -> None:\n",
        "        super().__init__(data, indices)\n",
        "        self._tagger = konlpy.tag.Mecab()\n",
        "\n",
        "    def _tokenize_with_position(self, sentence: str) -> List[Tuple[str, Tuple[int, int]]]:\n",
        "        position = 0\n",
        "        tokens = []\n",
        "        for morph in self._tagger.morphs(sentence):\n",
        "            position = sentence.find(morph, position)\n",
        "            tokens.append((morph, (position, position + len(morph))))\n",
        "            position += len(morph)\n",
        "        return tokens\n",
        "            \n",
        "    def __getitem__(self, index: int) -> Dict[str, Any]:\n",
        "        sample = super().__getitem__(index)\n",
        "\n",
        "        context, position = zip(*self._tokenize_with_position(sample['context']))\n",
        "        context, position = list(context), list(position)\n",
        "        question = self._tagger.morphs(sample['question'])\n",
        "\n",
        "        if sample['answers'] is not None:\n",
        "            answers = []\n",
        "            for answer in sample['answers']:\n",
        "                for start, (position_start, position_end) in enumerate(position):\n",
        "                    if position_start <= answer['answer_start'] < position_end:\n",
        "                        break\n",
        "                else:\n",
        "                    print(context, answer)\n",
        "                    raise ValueError(\"No mathced start position\")\n",
        "\n",
        "                target = ''.join(answer['text'].split(' '))\n",
        "                source = ''\n",
        "                for end, morph in enumerate(context[start:], start):\n",
        "                    source += morph\n",
        "                    if target in source:\n",
        "                        break\n",
        "                else:\n",
        "                    print(context, answer)\n",
        "                    raise ValueError(\"No Matched end position\")\n",
        "\n",
        "                answers.append({\n",
        "                    'start': start,\n",
        "                    'end': end\n",
        "                })\n",
        "        else:\n",
        "            answers = None\n",
        "        \n",
        "        return {\n",
        "            'guid': sample['guid'],\n",
        "            'context_original': sample['context'],\n",
        "            'context_position': position,\n",
        "            'question_original': sample['question'],\n",
        "            'context': context,\n",
        "            'question': question,\n",
        "            'answers': answers\n",
        "        }\n"
      ],
      "metadata": {
        "id": "AD3n-ek_4gwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QA Model 학습"
      ],
      "metadata": {
        "id": "LMNJiCT1Jg_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AlbertConfig, AlbertModel\n",
        "\n",
        "# # Initializing an ALBERT-base style configuration\n",
        "# albert_base_configuration = AlbertConfig(\n",
        "#     hidden_size=768,\n",
        "#     num_attention_heads=12,\n",
        "#     intermediate_size=3072,\n",
        "#     attention_probs_dropout_prob = 0.1\n",
        "# )\n",
        "\n",
        "# # Initializing a model from the ALBERT-base style configuration\n",
        "# #model = AlbertModel(albert_base_configuration)\n",
        "\n",
        "# torch.manual_seed(42)\n",
        "\n",
        "# # Accessing the model configuration\n",
        "# configuration = model.config"
      ],
      "metadata": {
        "id": "3MmJH6-8pDet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvZ7tz6viDHr"
      },
      "outputs": [],
      "source": [
        "\n",
        "# torch.manual_seed(42)\n",
        "\n",
        "# config = AlbertConfig(  ## 파라미터 변경할때는 여기서 파라미터 변경\n",
        "#      max_position_embeddings=512,\n",
        "#      hidden_size=768,\n",
        "#      num_attention_heads=12,\n",
        "#      intermediate_size=3072,\n",
        "#      attention_probs_dropout_prob = 0.1,\n",
        "#      hidden_dropout_prob   = 0.1\n",
        "# )\n",
        "# model = AlbertForQuestionAnswering(config)\n",
        "# model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-r6X-kSmzKfX"
      },
      "outputs": [],
      "source": [
        "from transformers import (AlbertConfig, AlbertForQuestionAnswering\n",
        "                          )\n",
        "\n",
        "# model = RobertaForQuestionAnswering.from_pretrained('xlm-roberta-base', max_length = 1024)\n",
        "# model = AutoModelForQuestionAnswering.from_pretrained('xlm-roberta-base', max_length = 1024)\n",
        "\n",
        "\n",
        "#model = AlbertForQuestionAnswering.from_pretrained(model_name, max_length = 512)\n",
        "\n",
        "\n",
        "model = AlbertForQuestionAnswering.from_pretrained(\n",
        "        pretrained_model_name_or_path=model_name,\n",
        "        max_length = 512,\n",
        "        attention_probs_dropout_prob=0.1,\n",
        "        hidden_dropout_prob=0.1\n",
        "    )\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "awKf-P0NxJ_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7lzoNgF0qSju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GYVgHjfzrZ2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qflLYUaIjbFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KWuaQ0M0kiZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.train() \n",
        "learning_rate = 1e-4 # 1e-5\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "ZJZnjjgFx3Sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_loss = pd.DataFrame(columns = ['epoch','train_loss','dev_loss'])\n",
        "df_loss"
      ],
      "metadata": {
        "id": "MGHYhEdnqm8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: 0.75 ** epoch)"
      ],
      "metadata": {
        "id": "iMlzHquJNchx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wZLTA8bnyRTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TGDoQ5Id9Et"
      },
      "outputs": [],
      "source": [
        "from statistics import mean\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "train_epoch = 20\n",
        "lowest_valid_loss = 9999.\n",
        "\n",
        "train_losses = []\n",
        "dev_losses = []\n",
        "\n",
        "step = 0\n",
        "\n",
        "model.train()\n",
        "for epoch in range(train_epoch):\n",
        "    print(\"Epoch\", epoch)\n",
        "    running_loss = 0.\n",
        "    losses = []\n",
        "    progress_bar = tqdm(train_loader, desc='Train')\n",
        "    for batch in progress_bar:\n",
        "        inputs = {key: value.cuda() for key, value in batch.items()}\n",
        "        output = model(**inputs)        \n",
        "\n",
        "        loss = output.loss\n",
        "        if not torch.isfinite(loss):\n",
        "            print('WARNING: non-finite loss, ending training ')\n",
        "            exit(1)\n",
        "\n",
        "        (loss / accumulation).backward()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        step += 1\n",
        "\n",
        "        if step % accumulation:\n",
        "            continue\n",
        "\n",
        "        clip_grad_norm_(model.parameters(), max_norm=1.)\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        \n",
        "        losses.append(running_loss / accumulation)\n",
        "        running_loss = 0.\n",
        "        progress_bar.set_description(f\"Train - Loss: {losses[-1]:.3f}\")\n",
        "    \n",
        "    #scheduler.step() # you can set it like this!\n",
        "    train_losses.append(mean(losses))\n",
        "    print(f\"train score: {train_losses[-1]:.3f}\")\n",
        "\n",
        "    # Evaluation\n",
        "    losses = []\n",
        "    for batch in tqdm(dev_loader, desc=\"Evaluation\"):        \n",
        "        with torch.no_grad():\n",
        "            inputs = {key: value.cuda() for key, value in batch.items()}\n",
        "            output = model(**inputs)        \n",
        "\n",
        "            loss = output.loss\n",
        "\n",
        "        losses.append(loss.item())\n",
        "    \n",
        "    #scheduler.step(mean(losses))\n",
        "    dev_losses.append(mean(losses))\n",
        "    print(f\"Evaluation score: {dev_losses[-1]:.3f}\")      \n",
        "\n",
        "\n",
        "model.save_pretrained(f'/content/drive/MyDrive/구름 자연어처리과정/프로젝트/Reading Comprehension/dump/Albert_model.{epoch}')\n",
        "\n",
        "df_loss['train_loss'] = train_losses\n",
        "df_loss['dev_loss'] = dev_losses\n",
        "df_loss.to_csv('albert_20_loss_Dropout.csv',index=False) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "h7IccSkw5IwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1vWKjGCwFwWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "McmFCB2TG3qJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_loss"
      ],
      "metadata": {
        "id": "0MaiFCElMnz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eOBb4qhpNvHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MsUbUyu5TfQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nmwZaOvFUmkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SDINjRuLaWt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rUJc0RL5beBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "692P_T1mhOK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NFEsKGESiVeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "O4vg1R0Q-45o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CpL7xU-UAANP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_loss.to_csv('/content/drive/MyDrive/구름 자연어처리과정/프로젝트/Reading Comprehension/사전 평가용/albert_30_loss_No_dropout.csv',index=False)"
      ],
      "metadata": {
        "id": "nPd94ckdGfGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5d537ztj4BeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_loss['train_loss'] = train_losses\n",
        "df_loss['dev_loss'] = dev_losses"
      ],
      "metadata": {
        "id": "cdE-dTZXJBOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "k0bVE6UYn5R5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_losses\n",
        "#dev_losses\n",
        "#train_dataset[0]"
      ],
      "metadata": {
        "id": "9Zsv5PG3Yi2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d70tUuX0gopx"
      },
      "source": [
        "## Answer Inference\n",
        "모델의 Output을 활용해서 질문의 답을 찾는 코드를 작성하자."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnsw0l0fgopx"
      },
      "outputs": [],
      "source": [
        "model = AlbertForQuestionAnswering.from_pretrained('/content/drive/MyDrive/구름 자연어처리과정/프로젝트/Reading Comprehension/dump/Albert_model_dropout')\n",
        "model.cuda()\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev_dataset[0]"
      ],
      "metadata": {
        "id": "RXVT6EdKdwiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PpLeQEOgopy"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYB-RE3Ygopy"
      },
      "source": [
        "# Test 출력 파일 작성"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "XiyxaozC0S8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XjwIZe5Ajhso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%\n",
        "from typing import List, Tuple, Dict, Any\n",
        "import json\n",
        "import random\n",
        "import re\n",
        "\n",
        "\n",
        "class KoMRC_for_test:\n",
        "    def __init__(self, data, indices: List[Tuple[int, int, int]]):\n",
        "        self._data = data\n",
        "        self._indices = indices\n",
        "\n",
        "    # Json을 불러오는 메소드\n",
        "    @classmethod\n",
        "    def load(cls, file_path: str):\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as fd:\n",
        "            data = json.load(fd)\n",
        "\n",
        "        indices = []\n",
        "        for d_id, document in enumerate(data[\"data\"]):\n",
        "            for p_id, paragraph in enumerate(document[\"paragraphs\"]):\n",
        "                for q_id, _ in enumerate(paragraph[\"qas\"]):\n",
        "                    indices.append((d_id, p_id, q_id))\n",
        "\n",
        "        return cls(data, indices)\n",
        "\n",
        "    # 데이터 셋을 잘라내는 메소드\n",
        "    @classmethod\n",
        "    def split(cls, dataset, eval_ratio: float = 0.1, seed=42):\n",
        "        indices = list(dataset._indices)\n",
        "        random.seed(seed)\n",
        "        random.shuffle(indices)\n",
        "        train_indices = indices[int(len(indices) * eval_ratio) :]\n",
        "        eval_indices = indices[: int(len(indices) * eval_ratio)]\n",
        "\n",
        "        return cls(dataset._data, train_indices), cls(dataset._data, eval_indices)\n",
        "\n",
        "    def __getitem__(self, index: int) -> Dict[str, Any]:\n",
        "        d_id, p_id, q_id = self._indices[index]\n",
        "        paragraph = self._data[\"data\"][d_id][\"paragraphs\"][p_id]\n",
        "        d_id, p_id, q_id = self._indices[index]\n",
        "        \n",
        "        # \\n, \\n\\n 을 찾아서 공백으로 처리\n",
        "        # \\n을 찾기 때문에 \\n\\n인 경우 공백이 2개 생김.....\n",
        "        # tokenization했을 때 차이가 있는지 확인...\n",
        "        p = re.compile(\"[\\\\n]\")\n",
        "        # p = re.compile(\"[#:^$@*※~&ㆍ!』\\\"\\'\\\\n…\\○]\")\n",
        "        context = paragraph[\"context\"]\n",
        "        context = p.sub(\" \", context)\n",
        "        qa = paragraph[\"qas\"][q_id]\n",
        "\n",
        "        \"\"\"\n",
        "        guid 부분\n",
        "        train dataset -- guid과\n",
        "        ai-hub train dataset -- context_id 이 다름\n",
        "        \"\"\"\n",
        "        # context_id 가 qas 안에 key 형태로 있음\n",
        "        if paragraph.get(\"context_id\"):\n",
        "            guid = str(paragraph[\"context_id\"])\n",
        "        else:\n",
        "            guid = qa[\"guid\"]\n",
        "\n",
        "        question = qa[\"question\"]\n",
        "\n",
        "        # answers에 필요한 것만 추출\n",
        "        if isinstance(qa['answers'], dict):\n",
        "            answers = [{\n",
        "                \"text\": qa[\"answers\"][\"text\"],\n",
        "                \"answer_start\": qa[\"answers\"][\"answer_start\"],\n",
        "            }]\n",
        "        else:\n",
        "            answers = qa[\"answers\"]\n",
        "\n",
        "        return {\n",
        "            \"guid\": guid,\n",
        "            \"context\": context,\n",
        "            \"question\": question,\n",
        "            \"answers\": answers,\n",
        "        }\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self._indices)"
      ],
      "metadata": {
        "id": "aab15BaJjg6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_path = '/content/drive/MyDrive/구름 자연어처리 프로젝트 공유 폴더/Reading Comprehension/data/divided/val.json'"
      ],
      "metadata": {
        "id": "37PJV8GGOYdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test_data = dev_dataset \n",
        "\n",
        "test_data = KoMRC_for_test.load(test_data_path)"
      ],
      "metadata": {
        "id": "tlu_BuhSZAPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data[0]"
      ],
      "metadata": {
        "id": "XHc3gJT1B066"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_validation_examples(examples, max_length, stride):\n",
        "    ids = [examples[i]['guid'] for i in range(len(examples))]\n",
        "    contexts = [examples[i]['context'] for i in range(len(examples))]\n",
        "    questions = [examples[i]['question'] for i in range(len(examples))]\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        contexts,\n",
        "        max_length=max_length,\n",
        "        truncation=\"only_second\",\n",
        "        stride=stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
        "    example_ids = []\n",
        "\n",
        "    for i in range(len(inputs[\"input_ids\"])):\n",
        "        sample_idx = sample_map[i]\n",
        "        example_ids.append(ids[sample_idx])\n",
        "\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "        offset = inputs[\"offset_mapping\"][i]\n",
        "        inputs[\"offset_mapping\"][i] = [\n",
        "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
        "        ]\n",
        "\n",
        "    inputs[\"example_id\"] = example_ids\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "g0urlcMdLbEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_encodings = preprocess_validation_examples(test_data, 512, 50)"
      ],
      "metadata": {
        "id": "sS9_uuW0L4AY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_encodings[0]"
      ],
      "metadata": {
        "id": "S3Wz6TbV7BF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_id = test_encodings.pop('example_id')\n",
        "offset_mapping = test_encodings.pop('offset_mapping')"
      ],
      "metadata": {
        "id": "nEvNZtR26R0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "test_dataset = MyDataset(test_encodings)"
      ],
      "metadata": {
        "id": "uAFI-tKw6RyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset[0]"
      ],
      "metadata": {
        "id": "yyB-oCJv7n7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "accumulation = 4\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size//accumulation, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "LaLiyOyDG7jD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "start_logits = None\n",
        "end_logits = None\n",
        "for batch in tqdm(test_loader, desc=\"test\"):        \n",
        "    with torch.no_grad():\n",
        "        inputs = {key: value.cuda() for key, value in batch.items()}\n",
        "        outputs = model(**inputs)        \n",
        "\n",
        "        loss = outputs.loss\n",
        "\n",
        "    if start_logits is None and end_logits is  None:\n",
        "        start_logits = outputs.start_logits.cpu().numpy()\n",
        "        end_logits = outputs.end_logits.cpu().numpy()\n",
        "    else:\n",
        "        start_logits = np.append(start_logits, outputs.start_logits.cpu().numpy(), axis=0)\n",
        "        end_logits = np.append(end_logits, outputs.end_logits.cpu().numpy(), axis=0)\n"
      ],
      "metadata": {
        "id": "4M5fxxwr4Pls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(start_logits)"
      ],
      "metadata": {
        "id": "z30pFJI6GtUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(start_logits))\n",
        "# print(len(end_logits))\n",
        "#print(start_logits[21928])\n",
        "print(end_logits[:5])"
      ],
      "metadata": {
        "id": "K0R7jtyw5U2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#example_id[-10:]"
      ],
      "metadata": {
        "id": "G1pO2t35GQHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "example_to_features = collections.defaultdict(list)\n",
        "for idx, id in enumerate(example_id):\n",
        "    example_to_features[id].append(idx)"
      ],
      "metadata": {
        "id": "ohTJS6Yo7xoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_id[:5]"
      ],
      "metadata": {
        "id": "XmhY39OY77YZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(start_logits)\n",
        "# print(end_logits[0])\n",
        "# print(offset_mapping[0])"
      ],
      "metadata": {
        "id": "5KwdHwbe9mPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_to_features"
      ],
      "metadata": {
        "id": "kPIVw1TyJnFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_logits[0]"
      ],
      "metadata": {
        "id": "k-h3MQAmJg0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "offset_mapping[0]"
      ],
      "metadata": {
        "id": "ObSLrXAKJqkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "n_best = 20\n",
        "max_answer_length = 100\n",
        "predicted_answers = []\n",
        "\n",
        "for example in test_data:\n",
        "    example_id = example[\"guid\"]\n",
        "    context = example[\"context\"]\n",
        "    answers = []\n",
        "\n",
        "    for feature_index in example_to_features[example_id]:\n",
        "        start_logit = start_logits[feature_index]\n",
        "        end_logit = end_logits[feature_index]\n",
        "        offsets = offset_mapping[feature_index]\n",
        "\n",
        "        start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
        "        end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
        "        \n",
        "        for start_index in start_indexes:\n",
        "            for end_index in end_indexes:\n",
        "                # Skip answers that are not fully in the context\n",
        "                if offsets[start_index] is None or offsets[end_index] is None:\n",
        "                    continue\n",
        "                # Skip answers with a length that is either < 0 or > max_answer_length.\n",
        "                if (\n",
        "                    end_index < start_index\n",
        "                    or end_index - start_index + 1 > max_answer_length\n",
        "                ):\n",
        "                    continue\n",
        "\n",
        "                answers.append(\n",
        "                    {\n",
        "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
        "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
        "                    }\n",
        "                )\n",
        "    if len(answers) == 0:\n",
        "        print('0')\n",
        "    else : best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
        "    predicted_answers.append({\"id\": example_id, \"prediction_text\": best_answer[\"text\"]})"
      ],
      "metadata": {
        "id": "UeMv4eEy8BPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UE5kaKsxC4nJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(predicted_answers)\n",
        "df"
      ],
      "metadata": {
        "id": "pdBDaBe1O37P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('/content/drive/MyDrive/구름 자연어처리과정/프로젝트/Reading Comprehension/사전 평가용/Albert_apoch20_Nodropout.csv',index=False)"
      ],
      "metadata": {
        "id": "KAlJMzZdO35P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_dataset[1]['context']"
      ],
      "metadata": {
        "id": "u3QZvlig31XS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_answer = pd.read_csv('/content/drive/MyDrive/구름 자연어처리 프로젝트 공유 폴더/Reading Comprehension/pre-evaluation/devset_정답.csv')\n",
        "df_answer "
      ],
      "metadata": {
        "id": "S-KISsJYO33N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_answers = [test_data[i]['answers'] for i in range(len(test_data))]"
      ],
      "metadata": {
        "id": "cqJUGnscDMpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_answers[0]"
      ],
      "metadata": {
        "id": "B4y35maoDIsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "H2TChiHUiooN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}